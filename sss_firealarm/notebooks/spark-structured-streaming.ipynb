{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Fire alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_topic = 'SmokeSensorEvent'\n",
    "temperature_topic = 'TemperatureSensorEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringified_smoke_df = smoke_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_smoke_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "smoke_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"smoke\", BooleanType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = stringified_smoke_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), smoke_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df.select(\"value.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore Spark Structured Streaming by example\n",
    "Please refer to [continuous-analytics-examples/epl_firealarm/readme.md](https://github.com/quantiaconsulting/continuous-analytics-examples/blob/master/epl_firealarm/readme.md) for the EPL version of the following queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_streaming_smoke_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_sdf=(raw_streaming_smoke_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), smoke_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query = (smoke_sdf\n",
    "    .writeStream\n",
    "    .format(\"memory\") # this is for debug purpose only! DO NOT USE IN PRODUCTION\n",
    "    .queryName(\"sinkTable\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following cell to see the most recent content of the sinkTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperarture_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])\n",
    "\n",
    "raw_streaming_temperature_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_topic)\n",
    "  .load())\n",
    "\n",
    "temperature_sdf = (raw_streaming_temperature_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0 - Filter\n",
    "\n",
    "the temperature events whose temperature is greater than 20 °C (was 50 °C in EPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the SQL style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logic table on top of the streaming data frame\n",
    "temperature_sdf.createTempView(\"TemperatureSensorEvent\")\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q0 = (spark.sql(\"select * from TemperatureSensorEvent where temperature > 20\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q0.stop()\n",
    "spark.catalog.dropTempView(\"TemperatureSensorEvent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0bis = (temperature_sdf\n",
    "                     .where(\"temperature > 20\") # you can add anything that fits in a SQL where statemente \n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0bis.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: there was no need to\n",
    "> * create a logic table on top of the streaming data frame\n",
    "> * drop such a logic table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Avg\n",
    "\n",
    "the average of all the temperature observation for each sensor up to the last event received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the SQL sytyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logic table on top of the streaming data frame\n",
    "temperature_sdf.createTempView(\"TemperatureSensorEvent\") # this time we will not clean it up, because we use it in the next queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the following query gives *intentionally* an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\"\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q1 = (spark.sql(query_string)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **append output mode** (i.e., the default one) is not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark, we need to use the **complete output mode**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\"\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q1 = (spark.sql(query_string)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .outputMode(\"complete\") # <-- CHANGE HERE\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable\").show(5) # woithout ORDER BY TS DESC because the result in the table is already only the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your query in SQL, register it and start it\n",
    "q1bis = (temperature_sdf#.withWatermark(\"ts\", \"10 seconds\") \n",
    "                     .groupBy(\"sensor\")\n",
    "                     .avg()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .outputMode(\"complete\") # \n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable\").show(5) # woithout ORDER BY TS DESC because the result in the table is already only the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q1bis.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Logical Sliding Window\n",
    "\n",
    "The average temperature observed by each sensor in the last 4 seconds\n",
    "\n",
    "MEMO: the average should change as soon as the receive a new event\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Logical Tumbling Window\n",
    "\n",
    "The average temperature of the last 30 seconds every 30 seconds (was 4 seconds in EPL)\n",
    "\n",
    "NOTE: this query is not possibile in the SQL style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = (temperature_sdf\n",
    "                         .withWatermark(\"TS\", \"30 seconds\")\n",
    "                         .groupBy(window(\"TS\", \"30 seconds\"),\"SENSOR\")\n",
    "                         .avg(\"TEMPERATURE\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Physical Sliding Window\n",
    "\n",
    "The moving average of the last 4 temperature events\n",
    "\n",
    "**Not supported**\n",
    "\n",
    "## Q5 - Physical Tumbling Window\n",
    "\n",
    "The moving average of the last 4 temperature events every 4 events \n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 - Logical Hopping Window\n",
    "\n",
    "The average temperature of the last 1 minute (was 4 seconds in EPL) every 5 seconds (was 2 seconds in EPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6 = (temperature_sdf\n",
    "      .withWatermark(\"TS\", \"1 minutes\")\n",
    "      .groupBy(window(\"TS\", \"1 minutes\", \"5 seconds\"),\"SENSOR\")\n",
    "      .avg(\"TEMPERATURE\")\n",
    "      .writeStream\n",
    "      .format(\"memory\")\n",
    "      .queryName(\"sinkTable\")\n",
    "      .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(5,False) # NOTE: here we order by window instead of ordering by timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the duplicates. They are present because the query is evalauted every 5 seconds, but a new event arrives every 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Stream-to-Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EPL, at this point we moved on to the pattern matching part required to satisfy the information need, i.e., \"find every smoke event followed by a temperature event whose temperature is above 50 °C within 2 minutes.\"\n",
    "\n",
    "Spark Structured Streaming does not support the EPL's operator `->` (that reads as *followed by*. We need to use a stream-to-stream join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the watermarks on event-time columns and the other two filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_smoke_events = (smoke_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .where(\"smoke = True\")\n",
    "                .withColumnRenamed(\"sensor\",\"sensorSmoke\")\n",
    "                .withColumnRenamed(\"ts\",\"tsSmoke\")\n",
    "               )\n",
    "\n",
    "last_minute_high_temperature_events = (temperature_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .where(\"temperature > 50\")\n",
    "                .withColumnRenamed(\"sensor\",\"sensorTemp\")\n",
    "                .withColumnRenamed(\"ts\",\"tsTemp\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf = (last_minute_smoke_events.join(\n",
    "  last_minute_high_temperature_events, expr(\"\"\"\n",
    "    (sensorTemp == sensorSmoke) AND\n",
    "    (tsTemp > tsSmoke ) AND\n",
    "    (tsTemp < tsSmoke + interval 1 minute )\n",
    "    \"\"\"\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = (join_sdf\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** To detect fire, run the appropriate cells in the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsTemp DESC\").show(5,False) # note, I change ts in tsTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look to the progesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import json\n",
    "while True:\n",
    "    print(json.dumps(q7.lastProgress, indent=4))\n",
    "    print(q7.status)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to interrupt the execution of the cell, prese the square icon in the bar or choose *interrupt kernel* from the *kernel* dropdown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This query is equivalent to the EPL pattern `every a = SmokeSensorEvent(smoke=true) -> every TemperatureSensorEvent(temperature > 50, sensor=a.sensor) where timer:within(1 min)`. \n",
    ">\n",
    "> Do not expect the same performances! It is evaluated as a relational join. Spark Structured Streaming lacks the specilized data structure of Esper.\n",
    ">\n",
    "> **It does not tame the torrent effect**, but this is expected! \n",
    ">\n",
    "> Spark Structured Streaming is a Data Stream Management System meant to tame *flow that you cannot stop*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counterintuitively, we can stop q7 because we only need the streaming Data Frame `join_sdf`. We do not need q7 to write its result in the in memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Count FireEvent\n",
    "\n",
    "we are very close to the solution of the running example, we \"just\" need to count the number of events generated by the previous query over an hopping window of 1 minutes that slides every 30 seconds (was a sliding window of 10 secondsin EPL). \n",
    "\n",
    "So let's count the results of Q7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8 = (join_sdf\n",
    "                     .withWatermark(\"tsTemp\", \"1 minutes\")\n",
    "                     .groupBy(window(\"tsTemp\", \"1 minutes\", \"30 seconds\"),\"sensorTemp\")\n",
    "                     .count()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\") \n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
