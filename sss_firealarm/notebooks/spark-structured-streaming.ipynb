{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Fire alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://9aef7c409332:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbaeb966460>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_topic = 'SmokeSensorEvent'\n",
    "temperature_topic = 'TemperatureSensorEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|    key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     0|2021-10-18 10:32:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     1|2021-10-18 10:32:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     2|2021-10-18 10:32:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     3|2021-10-18 10:32:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     4|2021-10-18 10:33:...|            0|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "|key|value                                             |\n",
      "+---+--------------------------------------------------+\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1634553145}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1634553156}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1634553166}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1634553176}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1634553186}|\n",
      "+---+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_smoke_df = smoke_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_smoke_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "smoke_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"smoke\", BooleanType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = stringified_smoke_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), smoke_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- sensor: string (nullable = true)\n",
      " |    |-- smoke: boolean (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2021-10-18 10:32:25|\n",
      "|    S1|false|2021-10-18 10:32:36|\n",
      "|    S1|false|2021-10-18 10:32:46|\n",
      "|    S1|false|2021-10-18 10:32:56|\n",
      "|    S1|false|2021-10-18 10:33:06|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.select(\"value.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore Spark Structured Streaming by example\n",
    "Please refer to [continuous-analytics-examples/epl_firealarm/readme.md](https://github.com/quantiaconsulting/continuous-analytics-examples/blob/master/epl_firealarm/readme.md) for the EPL version of the following queries.\n",
    "\n",
    "### Let's create the streaming Data Frames using the data in the kafka topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_streaming_smoke_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_streaming_smoke_df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_streaming_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_sdf=(raw_streaming_smoke_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), smoke_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- smoke: boolean (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: it is not a DataFrame, you cannot directly execute an action on it. The following cell *intetionally* gives an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();;\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2e418c6c2ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmoke_sdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();;\nkafka"
     ]
    }
   ],
   "source": [
    "smoke_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperarture_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])\n",
    "\n",
    "raw_streaming_temperature_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_topic)\n",
    "  .load())\n",
    "\n",
    "temperature_sdf = (raw_streaming_temperature_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to gain some confidence, let's first inspect the content of the stream of smoke event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query = (smoke_sdf\n",
    "    .writeStream\n",
    "    .format(\"memory\") # this is for debug purpose only! DO NOT USE IN PRODUCTION\n",
    "    .queryName(\"sinkTable\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '32000922-f1e6-4820-8391-94339da67bb3',\n",
       " 'runId': '1a5eb39a-6ab1-4959-8724-baedc7078cfb',\n",
       " 'name': 'sinkTable',\n",
       " 'timestamp': '2021-10-18T10:52:17.994Z',\n",
       " 'batchId': 29,\n",
       " 'numInputRows': 1,\n",
       " 'inputRowsPerSecond': 90.90909090909092,\n",
       " 'processedRowsPerSecond': 4.405286343612334,\n",
       " 'durationMs': {'addBatch': 126,\n",
       "  'getBatch': 0,\n",
       "  'latestOffset': 1,\n",
       "  'queryPlanning': 24,\n",
       "  'triggerExecution': 226,\n",
       "  'walCommit': 18},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'KafkaV2[Subscribe[SmokeSensorEvent]]',\n",
       "   'startOffset': {'SmokeSensorEvent': {'0': 119}},\n",
       "   'endOffset': {'SmokeSensorEvent': {'0': 120}},\n",
       "   'numInputRows': 1,\n",
       "   'inputRowsPerSecond': 90.90909090909092,\n",
       "   'processedRowsPerSecond': 4.405286343612334}],\n",
       " 'sink': {'description': 'MemorySink', 'numOutputRows': 1}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_query.lastProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the following cell to see the most recent content of the sinkTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2021-10-18 12:08:22|\n",
      "|    S1|false|2021-10-18 12:08:12|\n",
      "|    S1|false|2021-10-18 12:08:02|\n",
      "|    S1|false|2021-10-18 12:07:52|\n",
      "|    S1|false|2021-10-18 12:07:42|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do not forget to stop queries that you are not using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0 - Filter\n",
    "\n",
    "the temperature events whose temperature is greater than 20 °C (was 50 °C in EPL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the SQL style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logic table on top of the streaming data frame\n",
    "temperature_sdf.createTempView(\"TemperatureSensorEvent\")\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q0 = (spark.sql(\"select * from TemperatureSensorEvent where temperature > 20\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's ask for the execution plan, we will compare it with cells down with the one of the query in Data Frame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@23d5689\n",
      "+- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).sensor AS sensor#195, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).temperature AS temperature#196, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).ts AS ts#197]\n",
      "   +- Filter (from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).temperature > 20.0)\n",
      "      +- *(1) Project [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185]\n",
      "         +- MicroBatchScan[key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q0.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------------+\n",
      "|sensor|       temperature|                 ts|\n",
      "+------+------------------+-------------------+\n",
      "|    S1|22.179079291710003|2021-10-18 12:11:14|\n",
      "|    S1| 20.29315824114893|2021-10-18 12:11:04|\n",
      "|    S1|21.832024622651204|2021-10-18 12:10:34|\n",
      "|    S1| 21.75734312250273|2021-10-18 12:10:24|\n",
      "|    S1| 20.40437843675412|2021-10-18 12:10:14|\n",
      "+------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q0.stop()\n",
    "spark.catalog.dropTempView(\"TemperatureSensorEvent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0bis = (temperature_sdf\n",
    "                     .where(\"temperature > 20\") # you can add anything that fits in a SQL where statemente \n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's ask for the explanation of the plan. Comparing with the one of the SQL style, you can see that there is no difference. This is expected because the [catalyst optimizer](https://databricks.com/glossary/catalyst-optimizer) created it out of our declarations (which are semantically equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "WriteToDataSourceV2 org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@43513fd9\n",
      "+- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).sensor AS sensor#195, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).temperature AS temperature#196, from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).ts AS ts#197]\n",
      "   +- Filter (from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)).temperature > 20.0)\n",
      "      +- *(1) Project [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185]\n",
      "         +- MicroBatchScan[key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185] class org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q0bis.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------------+\n",
      "|sensor|       temperature|                 ts|\n",
      "+------+------------------+-------------------+\n",
      "|    S1| 20.45088665963757|2021-10-18 12:12:24|\n",
      "|    S1|22.179079291710003|2021-10-18 12:11:14|\n",
      "|    S1| 20.29315824114893|2021-10-18 12:11:04|\n",
      "|    S1|21.832024622651204|2021-10-18 12:10:34|\n",
      "|    S1| 21.75734312250273|2021-10-18 12:10:24|\n",
      "+------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0bis.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: there was no need to\n",
    "> * create a logic table on top of the streaming data frame\n",
    "> * drop such a logic table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Avg\n",
    "\n",
    "the average of all the temperature observation for each sensor up to the last event received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the SQL sytyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a logic table on top of the streaming data frame\n",
    "temperature_sdf.createTempView(\"TemperatureSensorEvent\") # this time we will not clean it up, because we use it in the next queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: the following query gives *intentionally* an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [SENSOR#195], [SENSOR#195, avg(temperature#196) AS avg(temperature)#1173]\n+- SubqueryAlias temperaturesensorevent\n   +- Project [value#193.sensor AS sensor#195, value#193.temperature AS temperature#196, value#193.ts AS ts#197]\n      +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)) AS value#193]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@62b2bce5, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6047cf1e, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@612ef3f5,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#172, value#173, topic#174, partition#175, offset#176L, timestamp#177, timestampType#178]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2bfd283c8455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# write your query in SQL, register it and start it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m q1 = (spark.sql(query_string)\n\u001b[0m\u001b[1;32m      9\u001b[0m                      \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                      \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueryName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;;\nAggregate [SENSOR#195], [SENSOR#195, avg(temperature#196) AS avg(temperature)#1173]\n+- SubqueryAlias temperaturesensorevent\n   +- Project [value#193.sensor AS sensor#195, value#193.temperature AS temperature#196, value#193.ts AS ts#197]\n      +- Project [from_json(StructField(sensor,StringType,true), StructField(temperature,DoubleType,true), StructField(ts,TimestampType,true), cast(value#180 as string), Some(Etc/UTC)) AS value#193]\n         +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@62b2bce5, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6047cf1e, org.apache.spark.sql.util.CaseInsensitiveStringMap@24f684c4, [key#179, value#180, topic#181, partition#182, offset#183L, timestamp#184, timestampType#185], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@612ef3f5,kafka,List(),None,List(),None,Map(startingOffsets -> earliest, subscribe -> TemperatureSensorEvent, kafka.bootstrap.servers -> kafka:9092),None), kafka, [key#172, value#173, topic#174, partition#175, offset#176L, timestamp#177, timestampType#178]\n"
     ]
    }
   ],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\"\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q1 = (spark.sql(query_string)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **append output mode** (i.e., the default one) is not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark, we need to use the **complete output mode**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\"\n",
    "\n",
    "# write your query in SQL, register it and start it\n",
    "q1 = (spark.sql(query_string)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .outputMode(\"complete\") # <-- CHANGE HERE\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|SENSOR| avg(temperature)|\n",
      "+------+-----------------+\n",
      "|    S1|19.98557651555871|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable\").show(5) # without ORDER BY TS DESC because the result in the table is already only the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DataFrame style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your query in SQL, register it and start it\n",
    "q1bis = (temperature_sdf#.withWatermark(\"ts\", \"10 seconds\") \n",
    "                     .groupBy(\"sensor\")\n",
    "                     .avg()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .outputMode(\"complete\") # \n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|sensor|  avg(temperature)|\n",
      "+------+------------------+\n",
      "|    S1|19.971543405362056|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# look up the most recent results\n",
    "spark.sql(\"SELECT * FROM sinkTable\").show(5) # woithout ORDER BY TS DESC because the result in the table is already only the most recent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up\n",
    "q1bis.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Logical Sliding Window\n",
    "\n",
    "The average temperature observed by each sensor in the last 4 seconds\n",
    "\n",
    "MEMO: the average should change as soon as the receive a new event\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Logical Tumbling Window\n",
    "\n",
    "The average temperature of the last 30 seconds every 30 seconds (was 4 seconds in EPL)\n",
    "\n",
    "NOTE: this query is not possibile in the SQL style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = (temperature_sdf\n",
    "                         .withWatermark(\"TS\", \"30 seconds\")\n",
    "                         .groupBy(window(\"TS\", \"30 seconds\"),\"SENSOR\")\n",
    "                         .avg(\"TEMPERATURE\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2021-10-18 12:23:30, 2021-10-18 12:24:00]|S1    |19.42756865944047 |\n",
      "|[2021-10-18 12:23:00, 2021-10-18 12:23:30]|S1    |20.786424195844763|\n",
      "|[2021-10-18 12:22:30, 2021-10-18 12:23:00]|S1    |18.306454897202062|\n",
      "|[2021-10-18 12:22:00, 2021-10-18 12:22:30]|S1    |19.751798320399647|\n",
      "|[2021-10-18 12:21:30, 2021-10-18 12:22:00]|S1    |20.13249798474993 |\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(5,False) # NOTE: here we order by window instead of ordering by timestamp# window instead of timestamp, again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Physical Sliding Window\n",
    "\n",
    "The moving average of the last 4 temperature events\n",
    "\n",
    "**Not supported**\n",
    "\n",
    "## Q5 - Physical Tumbling Window\n",
    "\n",
    "The moving average of the last 4 temperature events every 4 events \n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 - Logical Hopping Window\n",
    "\n",
    "The average temperature of the last 1 minute (was 4 seconds in EPL) every 5 seconds (was 2 seconds in EPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6 = (temperature_sdf\n",
    "      .withWatermark(\"TS\", \"1 minutes\")\n",
    "      .groupBy(window(\"TS\", \"1 minutes\", \"5 seconds\"),\"SENSOR\")\n",
    "      .avg(\"TEMPERATURE\")\n",
    "      .writeStream\n",
    "      .format(\"memory\")\n",
    "      .queryName(\"sinkTable\")\n",
    "      .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2021-10-18 12:25:35, 2021-10-18 12:26:35]|S1    |20.066321580129678|\n",
      "|[2021-10-18 12:25:30, 2021-10-18 12:26:30]|S1    |20.066321580129678|\n",
      "|[2021-10-18 12:25:25, 2021-10-18 12:26:25]|S1    |19.79773853431053 |\n",
      "|[2021-10-18 12:25:20, 2021-10-18 12:26:20]|S1    |19.79773853431053 |\n",
      "|[2021-10-18 12:25:15, 2021-10-18 12:26:15]|S1    |19.802156398351276|\n",
      "|[2021-10-18 12:25:10, 2021-10-18 12:26:10]|S1    |19.802156398351276|\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(6,False) # NOTE: here we order by window instead of ordering by timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the duplicates. They are present because the query is evalauted every 5 seconds, but a new event arrives every 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "q6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Stream-to-Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EPL, at this point we moved on to the pattern matching part required to satisfy the information need, i.e., \"find every smoke event followed by a temperature event whose temperature is above 50 °C within 2 minutes.\"\n",
    "\n",
    "Spark Structured Streaming does not support the EPL's operator `->` (that reads as *followed by*. We need to use a stream-to-stream join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the watermarks on event-time columns and the other two filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_smoke_events = (smoke_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .where(\"smoke = True\")\n",
    "                .withColumnRenamed(\"sensor\",\"sensorSmoke\")\n",
    "                .withColumnRenamed(\"ts\",\"tsSmoke\")\n",
    "               )\n",
    "\n",
    "last_minute_high_temperature_events = (temperature_sdf\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .where(\"temperature > 50\")\n",
    "                .withColumnRenamed(\"sensor\",\"sensorTemp\")\n",
    "                .withColumnRenamed(\"ts\",\"tsTemp\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sdf = (last_minute_smoke_events.join(\n",
    "  last_minute_high_temperature_events, expr(\"\"\"\n",
    "    (sensorTemp == sensorSmoke) AND\n",
    "    (tsTemp > tsSmoke ) AND\n",
    "    (tsTemp < tsSmoke + interval 2 minute )\n",
    "    \"\"\"\n",
    "    )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7 = (join_sdf\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** To detect fire, run the appropriate cells in the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------------------+----------+------------------+-------------------+\n",
      "|sensorSmoke|smoke|tsSmoke            |sensorTemp|temperature       |tsTemp             |\n",
      "+-----------+-----+-------------------+----------+------------------+-------------------+\n",
      "|S1         |true |2021-10-18 12:41:18|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:42:18|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:41:58|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:42:08|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:41:08|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:40:28|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:41:38|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:41:48|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:40:58|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:41:28|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:40:48|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:40:38|S1        |55.405213828949535|2021-10-18 12:42:20|\n",
      "|S1         |true |2021-10-18 12:40:38|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:41:58|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:40:58|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:41:48|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:40:18|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:40:48|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:40:28|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "|S1         |true |2021-10-18 12:41:08|S1        |56.10171131933996 |2021-10-18 12:42:10|\n",
      "+-----------+-----+-------------------+----------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY tsTemp DESC\").show(20,False) # note, I change ts in tsTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's have a look to the progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"ab342e77-f6c5-426f-930e-ba6425fc27d7\",\n",
      "    \"runId\": \"1a8522bc-7780-44fb-95cb-ece8f5b1051b\",\n",
      "    \"name\": \"sinkTable\",\n",
      "    \"timestamp\": \"2021-10-18T12:40:24.208Z\",\n",
      "    \"batchId\": 72,\n",
      "    \"numInputRows\": 0,\n",
      "    \"inputRowsPerSecond\": 0.0,\n",
      "    \"processedRowsPerSecond\": 0.0,\n",
      "    \"durationMs\": {\n",
      "        \"addBatch\": 3801,\n",
      "        \"getBatch\": 0,\n",
      "        \"latestOffset\": 1,\n",
      "        \"queryPlanning\": 107,\n",
      "        \"triggerExecution\": 3937,\n",
      "        \"walCommit\": 15\n",
      "    },\n",
      "    \"eventTime\": {\n",
      "        \"watermark\": \"2021-10-18T12:39:18.000Z\"\n",
      "    },\n",
      "    \"stateOperators\": [\n",
      "        {\n",
      "            \"numRowsTotal\": 25,\n",
      "            \"numRowsUpdated\": 0,\n",
      "            \"memoryUsedBytes\": 334768,\n",
      "            \"customMetrics\": {\n",
      "                \"loadedMapCacheHitCount\": 28800,\n",
      "                \"loadedMapCacheMissCount\": 0,\n",
      "                \"stateOnCurrentVersionSizeBytes\": 41168\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"sources\": [\n",
      "        {\n",
      "            \"description\": \"KafkaV2[Subscribe[SmokeSensorEvent]]\",\n",
      "            \"startOffset\": {\n",
      "                \"SmokeSensorEvent\": {\n",
      "                    \"0\": 768\n",
      "                }\n",
      "            },\n",
      "            \"endOffset\": {\n",
      "                \"SmokeSensorEvent\": {\n",
      "                    \"0\": 768\n",
      "                }\n",
      "            },\n",
      "            \"numInputRows\": 0,\n",
      "            \"inputRowsPerSecond\": 0.0,\n",
      "            \"processedRowsPerSecond\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"description\": \"KafkaV2[Subscribe[TemperatureSensorEvent]]\",\n",
      "            \"startOffset\": {\n",
      "                \"TemperatureSensorEvent\": {\n",
      "                    \"0\": 759\n",
      "                }\n",
      "            },\n",
      "            \"endOffset\": {\n",
      "                \"TemperatureSensorEvent\": {\n",
      "                    \"0\": 759\n",
      "                }\n",
      "            },\n",
      "            \"numInputRows\": 0,\n",
      "            \"inputRowsPerSecond\": 0.0,\n",
      "            \"processedRowsPerSecond\": 0.0\n",
      "        }\n",
      "    ],\n",
      "    \"sink\": {\n",
      "        \"description\": \"MemorySink\",\n",
      "        \"numOutputRows\": 0\n",
      "    }\n",
      "}\n",
      "{'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-51852bc8d4b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastProgress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq7\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import json\n",
    "while True:\n",
    "    print(json.dumps(q7.lastProgress, indent=4))\n",
    "    print(q7.status)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to interrupt the execution of the cell, prese the square icon in the bar or choose *interrupt kernel* from the *kernel* dropdown menu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This query is equivalent to the EPL pattern `every a = SmokeSensorEvent(smoke=true) -> every TemperatureSensorEvent(temperature > 50, sensor=a.sensor) where timer:within(1 min)`. \n",
    ">\n",
    "> Do not expect the same performances! It is evaluated as a relational join. Spark Structured Streaming lacks the specilized data structure of Esper.\n",
    ">\n",
    "> **It does not tame the torrent effect**, but this is expected! \n",
    ">\n",
    "> Spark Structured Streaming is a Data Stream Management System meant to tame *flow that you cannot stop*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counterintuitively, we can stop q7 because we only need the streaming Data Frame `join_sdf`. We do not need q7 to write its result in the in memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "q7.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Count FireEvent\n",
    "\n",
    "we are very close to the solution of the running example, we \"just\" need to count the number of events generated by the previous query over an hopping window of 1 minutes that slides every 30 seconds (was a sliding window of 10 secondsin EPL). \n",
    "\n",
    "So let's count the results of Q7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8 = (join_sdf\n",
    "                     .withWatermark(\"tsTemp\", \"1 minutes\")\n",
    "                     .groupBy(window(\"tsTemp\", \"1 minutes\", \"30 seconds\"),\"sensorTemp\")\n",
    "                     .count()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"sinkTable\") \n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------+-----+\n",
      "|window                                    |sensorTemp|count|\n",
      "+------------------------------------------+----------+-----+\n",
      "|[2021-10-18 12:48:30, 2021-10-18 12:49:30]|S1        |72   |\n",
      "|[2021-10-18 12:48:00, 2021-10-18 12:49:00]|S1        |72   |\n",
      "|[2021-10-18 12:47:30, 2021-10-18 12:48:30]|S1        |72   |\n",
      "|[2021-10-18 12:47:00, 2021-10-18 12:48:00]|S1        |72   |\n",
      "|[2021-10-18 12:46:30, 2021-10-18 12:47:30]|S1        |72   |\n",
      "|[2021-10-18 12:46:00, 2021-10-18 12:47:00]|S1        |72   |\n",
      "|[2021-10-18 12:45:30, 2021-10-18 12:46:30]|S1        |72   |\n",
      "|[2021-10-18 12:45:00, 2021-10-18 12:46:00]|S1        |72   |\n",
      "|[2021-10-18 12:44:30, 2021-10-18 12:45:30]|S1        |72   |\n",
      "|[2021-10-18 12:44:00, 2021-10-18 12:45:00]|S1        |72   |\n",
      "|[2021-10-18 12:43:30, 2021-10-18 12:44:30]|S1        |72   |\n",
      "|[2021-10-18 12:43:00, 2021-10-18 12:44:00]|S1        |72   |\n",
      "|[2021-10-18 12:42:30, 2021-10-18 12:43:30]|S1        |72   |\n",
      "|[2021-10-18 12:42:00, 2021-10-18 12:43:00]|S1        |72   |\n",
      "|[2021-10-18 12:41:30, 2021-10-18 12:42:30]|S1        |72   |\n",
      "|[2021-10-18 12:41:00, 2021-10-18 12:42:00]|S1        |72   |\n",
      "|[2021-10-18 12:40:30, 2021-10-18 12:41:30]|S1        |72   |\n",
      "|[2021-10-18 12:40:00, 2021-10-18 12:41:00]|S1        |60   |\n",
      "|[2021-10-18 12:39:30, 2021-10-18 12:40:30]|S1        |60   |\n",
      "|[2021-10-18 12:39:00, 2021-10-18 12:40:00]|S1        |72   |\n",
      "|[2021-10-18 12:38:30, 2021-10-18 12:39:30]|S1        |72   |\n",
      "|[2021-10-18 12:38:00, 2021-10-18 12:39:00]|S1        |72   |\n",
      "|[2021-10-18 12:37:30, 2021-10-18 12:38:30]|S1        |72   |\n",
      "|[2021-10-18 12:37:00, 2021-10-18 12:38:00]|S1        |66   |\n",
      "|[2021-10-18 12:36:30, 2021-10-18 12:37:30]|S1        |51   |\n",
      "|[2021-10-18 12:36:00, 2021-10-18 12:37:00]|S1        |26   |\n",
      "|[2021-10-18 12:35:30, 2021-10-18 12:36:30]|S1        |5    |\n",
      "+------------------------------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sinkTable ORDER BY window DESC\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "q8.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
