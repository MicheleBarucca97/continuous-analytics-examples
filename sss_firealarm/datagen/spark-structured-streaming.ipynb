{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Fire alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://434a97b7315b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcfabc6c490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_topic = 'SmokeSensorEvent'\n",
    "temperature_topic = 'TemperatureSensorEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|    key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     0|2020-11-24 15:34:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     1|2020-11-24 15:34:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     2|2020-11-24 15:35:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     3|2020-11-24 15:35:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     4|2020-11-24 15:35:...|            0|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "|key|value                                             |\n",
      "+---+--------------------------------------------------+\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606232086}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606232097}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606232107}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606232117}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606232127}|\n",
      "+---+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_smoke_df = smoke_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_smoke_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "smoke_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"smoke\", BooleanType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_smoke_df = stringified_smoke_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), smoke_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- sensor: string (nullable = true)\n",
      " |    |-- smoke: boolean (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2020-11-24 15:34:46|\n",
      "|    S1|false|2020-11-24 15:34:57|\n",
      "|    S1|false|2020-11-24 15:35:07|\n",
      "|    S1|false|2020-11-24 15:35:17|\n",
      "|    S1|false|2020-11-24 15:35:27|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_smoke_df.select(\"value.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "Please refer to [Gitter](https://gitter.im/USDE2020/EPL) for the EPL version of the following queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_smoke_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_streaming_smoke_df=(streaming_smoke_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), smoke_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_query = (decoded_streaming_smoke_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"SmokeSensorEvent\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2020-11-24 15:34:46|\n",
      "|    S1|false|2020-11-24 15:34:57|\n",
      "|    S1|false|2020-11-24 15:35:07|\n",
      "|    S1|false|2020-11-24 15:35:17|\n",
      "|    S1|false|2020-11-24 15:35:27|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM SmokeSensorEvent LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperarture_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])\n",
    "\n",
    "streaming_temperature_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_topic)\n",
    "  .load())\n",
    "\n",
    "decoded_streaming_temperature_df = (streaming_temperature_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))\n",
    "\n",
    "temperature_query = (decoded_streaming_temperature_df\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"TemperatureSensorEvent\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_temperature_df.select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\")).select(\"value.*\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------------+\n",
      "|sensor|       temperature|                 ts|\n",
      "+------+------------------+-------------------+\n",
      "|    S1|21.731105455964585|2020-11-24 15:34:31|\n",
      "|    S1| 20.45618833625183|2020-11-24 15:34:42|\n",
      "|    S1| 20.81828476345006|2020-11-24 15:35:02|\n",
      "|    S1|20.054285720118937|2020-11-24 15:35:12|\n",
      "|    S1| 20.28286479862641|2020-11-24 15:35:42|\n",
      "+------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM TemperatureSensorEvent WHERE temperature > 20\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2020-11-24 15:34:46|\n",
      "|    S1|false|2020-11-24 15:34:57|\n",
      "|    S1|false|2020-11-24 15:35:07|\n",
      "|    S1|false|2020-11-24 15:35:17|\n",
      "|    S1|false|2020-11-24 15:35:27|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM SmokeSensorEvent\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1| true|2020-11-24 15:40:26|\n",
      "|    S1| true|2020-11-24 15:40:36|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM SmokeSensorEvent WHERE smoke\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|SENSOR| avg(temperature)|\n",
      "+------+-----------------+\n",
      "|    S1|19.71852147462904|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Logical Sliding Window\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Logical Tumbling Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query = (decoded_streaming_temperature_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"1 minutes\"),\"SENSOR\")\n",
    "                         .avg(\"TEMPERATURE\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2020-11-24 15:34:00, 2020-11-24 15:35:00]|S1    |20.700160450359075|\n",
      "|[2020-11-24 15:35:00, 2020-11-24 15:36:00]|S1    |19.71717485127529 |\n",
      "|[2020-11-24 15:36:00, 2020-11-24 15:37:00]|S1    |19.01932177618036 |\n",
      "|[2020-11-24 15:37:00, 2020-11-24 15:38:00]|S1    |19.511754399001067|\n",
      "|[2020-11-24 15:38:00, 2020-11-24 15:39:00]|S1    |19.51523745625821 |\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY window DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 - Physical Sliding Window\n",
    "\n",
    "**Not supported**\n",
    "\n",
    "## Q6 - Physical Tumbling Window\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Logical Hopping Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHW_temperature_query = (decoded_streaming_temperature_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"1 minutes\", \"5 seconds\"),\"SENSOR\")\n",
    "                         .avg(\"TEMPERATURE\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2020-11-24 15:33:35, 2020-11-24 15:34:35]|S1    |21.731105455964585|\n",
      "|[2020-11-24 15:33:40, 2020-11-24 15:34:40]|S1    |21.731105455964585|\n",
      "|[2020-11-24 15:33:45, 2020-11-24 15:34:45]|S1    |21.093646896108208|\n",
      "|[2020-11-24 15:33:50, 2020-11-24 15:34:50]|S1    |21.093646896108208|\n",
      "|[2020-11-24 15:33:55, 2020-11-24 15:34:55]|S1    |20.700160450359075|\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY window DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHW_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Stream-to-Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this stream-to-stream join is equivalent to the EPL pattern `every a = SmokeSensorEvent(smoke=true) -> every TemperatureSensorEvent(temperature > 50, sensor=a.sensor) where timer:within(1 min)`. Do not expect the same performances! It is evaluated differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply watermarks on event-time columns and other filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_smoke_events = (decoded_streaming_smoke_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"smoke\") == True)\n",
    "               )\n",
    "\n",
    "last_minute_high_temperature_events = (decoded_streaming_temperature_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"temperature\") > 50)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = (last_minute_smoke_events.join(\n",
    "  last_minute_high_temperature_events,\n",
    "    (last_minute_smoke_events.sensor == last_minute_high_temperature_events.sensor) &\n",
    "    (last_minute_smoke_events.ts < last_minute_high_temperature_events.ts))\n",
    "           .select(last_minute_smoke_events.sensor,\n",
    "                   last_minute_smoke_events.smoke,\n",
    "                   last_minute_high_temperature_events.temperature,\n",
    "                   last_minute_smoke_events.ts\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query = (join_df\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------+-------------------+\n",
      "|sensor|smoke|temperature       |ts                 |\n",
      "+------+-----+------------------+-------------------+\n",
      "|S1    |true |54.831176555315075|2020-11-24 15:40:26|\n",
      "|S1    |true |54.831176555315075|2020-11-24 15:40:36|\n",
      "|S1    |true |54.831176555315075|2020-11-24 15:40:46|\n",
      "|S1    |true |54.831176555315075|2020-11-24 15:40:56|\n",
      "|S1    |true |54.831176555315075|2020-11-24 15:41:06|\n",
      "+------+-----+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** To detect fire, run the appropriate cells in the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 - Count FireEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Fire_Event_query = (join_df\n",
    "                     .withWatermark(\"TS\", \"1 minutes\")\n",
    "                     .groupBy(window(\"TS\", \"1 minutes\", \"30 seconds\"),\"SENSOR\")\n",
    "                     .count()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+-----+\n",
      "|window                                    |SENSOR|count|\n",
      "+------------------------------------------+------+-----+\n",
      "|[2020-11-24 15:50:30, 2020-11-24 15:51:30]|S1    |63   |\n",
      "|[2020-11-24 15:50:00, 2020-11-24 15:51:00]|S1    |63   |\n",
      "|[2020-11-24 15:49:30, 2020-11-24 15:50:30]|S1    |63   |\n",
      "|[2020-11-24 15:49:00, 2020-11-24 15:50:00]|S1    |63   |\n",
      "|[2020-11-24 15:48:30, 2020-11-24 15:49:30]|S1    |63   |\n",
      "|[2020-11-24 15:48:00, 2020-11-24 15:49:00]|S1    |63   |\n",
      "|[2020-11-24 15:47:30, 2020-11-24 15:48:30]|S1    |63   |\n",
      "|[2020-11-24 15:47:00, 2020-11-24 15:48:00]|S1    |63   |\n",
      "|[2020-11-24 15:46:30, 2020-11-24 15:47:30]|S1    |63   |\n",
      "|[2020-11-24 15:46:00, 2020-11-24 15:47:00]|S1    |63   |\n",
      "|[2020-11-24 15:45:30, 2020-11-24 15:46:30]|S1    |63   |\n",
      "|[2020-11-24 15:45:00, 2020-11-24 15:46:00]|S1    |63   |\n",
      "|[2020-11-24 15:44:30, 2020-11-24 15:45:30]|S1    |63   |\n",
      "|[2020-11-24 15:44:00, 2020-11-24 15:45:00]|S1    |78   |\n",
      "|[2020-11-24 15:43:30, 2020-11-24 15:44:30]|S1    |84   |\n",
      "|[2020-11-24 15:43:00, 2020-11-24 15:44:00]|S1    |84   |\n",
      "|[2020-11-24 15:42:30, 2020-11-24 15:43:30]|S1    |84   |\n",
      "|[2020-11-24 15:42:00, 2020-11-24 15:43:00]|S1    |84   |\n",
      "|[2020-11-24 15:41:30, 2020-11-24 15:42:30]|S1    |84   |\n",
      "|[2020-11-24 15:41:00, 2020-11-24 15:42:00]|S1    |84   |\n",
      "|[2020-11-24 15:40:30, 2020-11-24 15:41:30]|S1    |84   |\n",
      "|[2020-11-24 15:40:00, 2020-11-24 15:41:00]|S1    |56   |\n",
      "|[2020-11-24 15:39:30, 2020-11-24 15:40:30]|S1    |14   |\n",
      "+------------------------------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY window DESC\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Fire_Event_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
