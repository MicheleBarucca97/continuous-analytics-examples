{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Fire alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://e6e69d9ae091:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fba1e706430>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_topic = 'SmokeSensorEvent'\n",
    "temperature_topic = 'TemperatureSensorEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|    key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     0|2020-11-24 16:19:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     1|2020-11-24 16:19:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     2|2020-11-24 16:19:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     3|2020-11-24 16:19:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|SmokeSensorEvent|        0|     4|2020-11-24 16:20:...|            0|\n",
      "+-------+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "smoke_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------+\n",
      "|key|value                                             |\n",
      "+---+--------------------------------------------------+\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606234762}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606234773}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606234783}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606234793}|\n",
      "|S1 |{\"sensor\": \"S1\", \"smoke\": false, \"ts\": 1606234803}|\n",
      "+---+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_smoke_df = smoke_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_smoke_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "smoke_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"smoke\", BooleanType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_smoke_df = stringified_smoke_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), smoke_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- sensor: string (nullable = true)\n",
      " |    |-- smoke: boolean (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2020-11-24 16:19:22|\n",
      "|    S1|false|2020-11-24 16:19:33|\n",
      "|    S1|false|2020-11-24 16:19:43|\n",
      "|    S1|false|2020-11-24 16:19:53|\n",
      "|    S1|false|2020-11-24 16:20:03|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_smoke_df.select(\"value.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "Please refer to [Gitter](https://gitter.im/USDE2020/EPL) for the EPL version of the following queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_smoke_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", smoke_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_streaming_smoke_df=(streaming_smoke_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), smoke_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- smoke: boolean (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_streaming_smoke_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_query = (decoded_streaming_smoke_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"SmokeSensorEvent\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2020-11-24 16:39:14|\n",
      "|    S1|false|2020-11-24 16:39:04|\n",
      "|    S1|false|2020-11-24 16:38:54|\n",
      "|    S1|false|2020-11-24 16:38:44|\n",
      "|    S1|false|2020-11-24 16:38:34|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM SmokeSensorEvent ORDER BY TS DESC\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperarture_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])\n",
    "\n",
    "streaming_temperature_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_topic)\n",
    "  .load())\n",
    "\n",
    "decoded_streaming_temperature_df = (streaming_temperature_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))\n",
    "\n",
    "temperature_query = (decoded_streaming_temperature_df\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"TemperatureSensorEvent\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_temperature_df.select(from_json(col(\"value\").cast(\"string\"), temperarture_schema).alias(\"value\")).select(\"value.*\").printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q0 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------------+\n",
      "|sensor|       temperature|                 ts|\n",
      "+------+------------------+-------------------+\n",
      "|    S1|20.007560270758102|2020-11-24 16:19:59|\n",
      "|    S1| 20.09222114500581|2020-11-24 16:20:10|\n",
      "|    S1| 20.50689495592087|2020-11-24 16:20:20|\n",
      "|    S1|20.763614705089402|2020-11-24 16:20:30|\n",
      "|    S1|21.459574875521923|2020-11-24 16:21:10|\n",
      "+------+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM TemperatureSensorEvent WHERE temperature > 20\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1|false|2020-11-24 16:19:22|\n",
      "|    S1|false|2020-11-24 16:19:33|\n",
      "|    S1|false|2020-11-24 16:19:43|\n",
      "|    S1|false|2020-11-24 16:19:53|\n",
      "|    S1|false|2020-11-24 16:20:03|\n",
      "+------+-----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM SmokeSensorEvent\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|sensor|smoke|                 ts|\n",
      "+------+-----+-------------------+\n",
      "|    S1| true|2020-11-24 16:40:39|\n",
      "|    S1| true|2020-11-24 16:40:49|\n",
      "|    S1| true|2020-11-24 16:40:59|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM SmokeSensorEvent WHERE smoke\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|SENSOR| avg(temperature)|\n",
      "+------+-----------------+\n",
      "|    S1|19.87929456823966|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT SENSOR, AVG(temperature) \n",
    "FROM TemperatureSensorEvent\n",
    "GROUP BY SENSOR\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Logical Sliding Window\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Logical Tumbling Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query = (decoded_streaming_temperature_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"1 minutes\"),\"SENSOR\")\n",
    "                         .avg(\"TEMPERATURE\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2020-11-24 16:42:00, 2020-11-24 16:43:00]|S1    |19.66152089883474 |\n",
      "|[2020-11-24 16:41:00, 2020-11-24 16:42:00]|S1    |19.442302437875963|\n",
      "|[2020-11-24 16:40:00, 2020-11-24 16:41:00]|S1    |19.680136023966956|\n",
      "|[2020-11-24 16:39:00, 2020-11-24 16:40:00]|S1    |20.018839642142996|\n",
      "|[2020-11-24 16:38:00, 2020-11-24 16:39:00]|S1    |19.976101350750596|\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY window DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 - Physical Sliding Window\n",
    "\n",
    "**Not supported**\n",
    "\n",
    "## Q6 - Physical Tumbling Window\n",
    "\n",
    "**Not supported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Logical Hopping Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHW_temperature_query = (decoded_streaming_temperature_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"1 minutes\", \"5 seconds\"),\"SENSOR\")\n",
    "                         .avg(\"TEMPERATURE\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------------------+\n",
      "|window                                    |SENSOR|avg(TEMPERATURE)  |\n",
      "+------------------------------------------+------+------------------+\n",
      "|[2020-11-24 16:44:40, 2020-11-24 16:45:40]|S1    |20.2465254488466  |\n",
      "|[2020-11-24 16:44:35, 2020-11-24 16:45:35]|S1    |20.2465254488466  |\n",
      "|[2020-11-24 16:44:30, 2020-11-24 16:45:30]|S1    |20.661297009034165|\n",
      "|[2020-11-24 16:44:25, 2020-11-24 16:45:25]|S1    |20.661297009034165|\n",
      "|[2020-11-24 16:44:20, 2020-11-24 16:45:20]|S1    |20.861393843699165|\n",
      "+------------------------------------------+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY window DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LHW_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Stream-to-Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this stream-to-stream join is equivalent to the EPL pattern `every a = SmokeSensorEvent(smoke=true) -> every TemperatureSensorEvent(temperature > 50, sensor=a.sensor) where timer:within(1 min)`. Do not expect the same performances! It is evaluated differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply watermarks on event-time columns and other filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_minute_smoke_events = (decoded_streaming_smoke_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"smoke\") == True)\n",
    "               )\n",
    "\n",
    "last_minute_high_temperature_events = (decoded_streaming_temperature_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"temperature\") > 50)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = (last_minute_smoke_events.join(\n",
    "  last_minute_high_temperature_events,\n",
    "    (last_minute_smoke_events.sensor == last_minute_high_temperature_events.sensor) &\n",
    "    (last_minute_smoke_events.ts < last_minute_high_temperature_events.ts))\n",
    "           .select(last_minute_smoke_events.sensor,\n",
    "                   last_minute_smoke_events.smoke,\n",
    "                   last_minute_high_temperature_events.temperature,\n",
    "                   last_minute_smoke_events.ts\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query = (join_df\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------+-------------------+\n",
      "|sensor|smoke|temperature       |ts                 |\n",
      "+------+-----+------------------+-------------------+\n",
      "|S1    |true |55.742657075291305|2020-11-24 16:54:10|\n",
      "|S1    |true |53.51279575442952 |2020-11-24 16:54:00|\n",
      "|S1    |true |55.742657075291305|2020-11-24 16:54:00|\n",
      "|S1    |true |53.51279575442952 |2020-11-24 16:53:50|\n",
      "|S1    |true |55.832399109094005|2020-11-24 16:53:50|\n",
      "+------+-----+------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY ts DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT** To detect fire, run the appropriate cells in the data generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 - Count FireEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Fire_Event_query = (join_df\n",
    "                     .withWatermark(\"TS\", \"1 minutes\")\n",
    "                     .groupBy(window(\"TS\", \"1 minutes\", \"30 seconds\"),\"SENSOR\")\n",
    "                     .count()\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+-----+\n",
      "|window                                    |SENSOR|count|\n",
      "+------------------------------------------+------+-----+\n",
      "|[2020-11-24 16:54:00, 2020-11-24 16:55:00]|S1    |69   |\n",
      "|[2020-11-24 16:53:30, 2020-11-24 16:54:30]|S1    |86   |\n",
      "|[2020-11-24 16:53:00, 2020-11-24 16:54:00]|S1    |95   |\n",
      "|[2020-11-24 16:52:30, 2020-11-24 16:53:30]|S1    |96   |\n",
      "|[2020-11-24 16:52:00, 2020-11-24 16:53:00]|S1    |96   |\n",
      "|[2020-11-24 16:51:30, 2020-11-24 16:52:30]|S1    |96   |\n",
      "|[2020-11-24 16:51:00, 2020-11-24 16:52:00]|S1    |96   |\n",
      "|[2020-11-24 16:50:30, 2020-11-24 16:51:30]|S1    |96   |\n",
      "|[2020-11-24 16:50:00, 2020-11-24 16:51:00]|S1    |96   |\n",
      "|[2020-11-24 16:49:30, 2020-11-24 16:50:30]|S1    |96   |\n",
      "|[2020-11-24 16:49:00, 2020-11-24 16:50:00]|S1    |96   |\n",
      "|[2020-11-24 16:48:30, 2020-11-24 16:49:30]|S1    |96   |\n",
      "|[2020-11-24 16:48:00, 2020-11-24 16:49:00]|S1    |96   |\n",
      "|[2020-11-24 16:47:30, 2020-11-24 16:48:30]|S1    |96   |\n",
      "|[2020-11-24 16:47:00, 2020-11-24 16:48:00]|S1    |96   |\n",
      "|[2020-11-24 16:46:30, 2020-11-24 16:47:30]|S1    |96   |\n",
      "|[2020-11-24 16:46:00, 2020-11-24 16:47:00]|S1    |96   |\n",
      "|[2020-11-24 16:45:30, 2020-11-24 16:46:30]|S1    |96   |\n",
      "|[2020-11-24 16:45:00, 2020-11-24 16:46:00]|S1    |96   |\n",
      "|[2020-11-24 16:44:30, 2020-11-24 16:45:30]|S1    |96   |\n",
      "|[2020-11-24 16:44:00, 2020-11-24 16:45:00]|S1    |96   |\n",
      "|[2020-11-24 16:43:30, 2020-11-24 16:44:30]|S1    |96   |\n",
      "|[2020-11-24 16:43:00, 2020-11-24 16:44:00]|S1    |96   |\n",
      "|[2020-11-24 16:42:30, 2020-11-24 16:43:30]|S1    |80   |\n",
      "|[2020-11-24 16:42:00, 2020-11-24 16:43:00]|S1    |80   |\n",
      "|[2020-11-24 16:41:30, 2020-11-24 16:42:30]|S1    |96   |\n",
      "|[2020-11-24 16:41:00, 2020-11-24 16:42:00]|S1    |96   |\n",
      "|[2020-11-24 16:40:30, 2020-11-24 16:41:30]|S1    |96   |\n",
      "|[2020-11-24 16:40:00, 2020-11-24 16:41:00]|S1    |48   |\n",
      "+------------------------------------------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY window DESC\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count_Fire_Event_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
