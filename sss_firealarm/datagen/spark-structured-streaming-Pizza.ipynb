{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Demo\n",
    "## Pizza Oven\n",
    "\n",
    "\n",
    "### Authors\n",
    "\n",
    "```\n",
    "Marco Balduini - marco.balduini@quantiaconsulting.com\n",
    "Emanuele Della Valle - emanuele.dellavalle@polimi.it\n",
    "```\n",
    "```\n",
    "Translation to SSS: Massimo Pavan - massimo1.pavan@mail.polimi.it\n",
    "```\n",
    "\n",
    "### Use Case Description - Linear Pizza Oven\n",
    "We have a linear oven to continuously cook pizza.\n",
    "\n",
    "The cooking operation has two main steps:\n",
    "\n",
    "* the cooking of the pizza base, and\n",
    "* the mozzarella melting area.\n",
    "\n",
    "There are two sensors:\n",
    "\n",
    "* S1 measures the temperature and the relative humidity of the pizza base cooking area.\n",
    "* S2 measures the temperature and the relative humidity of the mozzarella melting area. \n",
    "\n",
    "Both sensors send a temperature measurement every minute, but are not synchronised.\n",
    "\n",
    "Most of the functions used in this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://611d79dc0016:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f650e301400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import io\n",
    "from pyspark.sql.functions import *\n",
    "import time\n",
    "import json\n",
    "import struct\n",
    "import requests \n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.5,org.apache.kafka:kafka-clients:2.6.0 pyspark-shell'\n",
    "                                    \n",
    "spark = (SparkSession.builder \n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    "        )\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_topic = 'TemperatureHumiditySensorEvent'\n",
    "servers = \"kafka:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding spark-kafka integration\n",
    "Let's treat first kafka as a bulk source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_df = (spark\n",
    "  .read\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"subscribe\", temperature_humidity_topic)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"endingOffsets\", \"latest\")\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_humidity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|    key|               value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+-------+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "|[53 31]|[7B 22 73 65 6E 7...|TemperatureHumidi...|        0|     0|2021-02-26 15:33:...|            0|\n",
      "|[53 32]|[7B 22 73 65 6E 7...|TemperatureHumidi...|        0|     1|2021-02-26 15:33:...|            0|\n",
      "|[53 32]|[7B 22 73 65 6E 7...|TemperatureHumidi...|        0|     2|2021-02-26 15:33:...|            0|\n",
      "|[53 31]|[7B 22 73 65 6E 7...|TemperatureHumidi...|        0|     3|2021-02-26 15:33:...|            0|\n",
      "|[53 32]|[7B 22 73 65 6E 7...|TemperatureHumidi...|        0|     4|2021-02-26 15:33:...|            0|\n",
      "+-------+--------------------+--------------------+---------+------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_humidity_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------------------------------------------------+\n",
      "|key|value                                                                 |\n",
      "+---+----------------------------------------------------------------------+\n",
      "|S1 |{\"sensor\": \"S1\", \"temperature\": 290, \"humidity\": 30, \"ts\": 1614353606}|\n",
      "|S2 |{\"sensor\": \"S2\", \"temperature\": 105, \"humidity\": 55, \"ts\": 1614353610}|\n",
      "|S2 |{\"sensor\": \"S2\", \"temperature\": 110, \"humidity\": 60, \"ts\": 1614353613}|\n",
      "|S1 |{\"sensor\": \"S1\", \"temperature\": 305, \"humidity\": 38, \"ts\": 1614353616}|\n",
      "|S2 |{\"sensor\": \"S2\", \"temperature\": 120, \"humidity\": 65, \"ts\": 1614353619}|\n",
      "+---+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringified_temperature_humidity_df = temperature_humidity_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "stringified_temperature_humidity_df.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "temperature_humidity_schema = StructType([\n",
    "    StructField(\"sensor\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"humidity\", IntegerType(), True),\n",
    "    StructField(\"ts\", TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_temperature_humidity_df = stringified_temperature_humidity_df.select(col(\"key\").cast(\"string\"),from_json(col(\"value\"), temperature_humidity_schema).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: struct (nullable = true)\n",
      " |    |-- sensor: string (nullable = true)\n",
      " |    |-- temperature: integer (nullable = true)\n",
      " |    |-- humidity: integer (nullable = true)\n",
      " |    |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_temperature_humidity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S1|        290|      30|2021-02-26 15:33:26|\n",
      "|    S2|        105|      55|2021-02-26 15:33:30|\n",
      "|    S2|        110|      60|2021-02-26 15:33:33|\n",
      "|    S1|        305|      38|2021-02-26 15:33:36|\n",
      "|    S2|        120|      65|2021-02-26 15:33:39|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_temperature_humidity_df.select(\"value.*\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "Please refer to [insert_link_here_if_available]() for the EPL version of the following queries.\n",
    "\n",
    "link to docs: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_temperature_humidity_df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", servers)\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .option(\"subscribe\", temperature_humidity_topic)\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_streaming_temperature_humidity_df=(streaming_temperature_humidity_df\n",
    "                      .select(from_json(col(\"value\").cast(\"string\"), temperature_humidity_schema).alias(\"value\"))\n",
    "                      .select(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- temperature: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded_streaming_temperature_humidity_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_query = (decoded_streaming_temperature_humidity_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"temperature_humiditySensorEvent\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S1|        290|      30|2020-07-21 12:00:00|\n",
      "|    S1|        290|      30|2020-07-21 12:00:00|\n",
      "|    S2|        105|      55|2020-07-21 12:00:15|\n",
      "|    S2|        105|      55|2020-07-21 12:00:15|\n",
      "|    S2|        110|      60|2020-07-21 12:00:45|\n",
      "|    S2|        110|      60|2020-07-21 12:00:45|\n",
      "|    S1|        305|      38|2020-07-21 12:01:00|\n",
      "|    S1|        305|      38|2020-07-21 12:01:00|\n",
      "|    S2|        120|      65|2020-07-21 12:01:15|\n",
      "|    S2|        120|      65|2020-07-21 12:01:15|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent ORDER BY TS ASC\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S2|         95|      65|2021-02-26 15:33:57|\n",
      "|    S2|         90|      60|2021-02-26 15:34:00|\n",
      "|    S2|         95|      65|2021-02-27 09:20:58|\n",
      "|    S2|         90|      60|2021-02-27 09:21:01|\n",
      "|    S2|         95|      65|2021-03-01 11:37:13|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE temperature < 100 AND sensor = 'S2' \").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all the measurements in a given range\n",
    "### Absolute range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S1|        290|      30|2020-07-21 12:00:00|\n",
      "|    S2|        105|      55|2020-07-21 12:00:15|\n",
      "|    S2|        110|      60|2020-07-21 12:00:45|\n",
      "|    S1|        305|      38|2020-07-21 12:01:00|\n",
      "|    S2|        120|      65|2020-07-21 12:01:15|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE ts >= '2020-07-21 12:00:00' AND ts <= '2020-07-21 12:05:00'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative range (start: -36h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S1|        290|      30|2021-03-01 11:36:42|\n",
      "|    S2|        105|      55|2021-03-01 11:36:46|\n",
      "|    S2|        110|      60|2021-03-01 11:36:49|\n",
      "|    S1|        305|      38|2021-03-01 11:36:52|\n",
      "|    S2|        120|      65|2021-03-01 11:36:55|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = int(time.time())\n",
    "thirtysixhoursago = datetime.fromtimestamp(now - 60*60*36).strftime(\"%Y-%m-%d %H:%M:%S\") #60*60*36 = seconds*minutes*hours\n",
    "query = \"SELECT * FROM temperature_humiditySensorEvent WHERE ts >= '{}'\".format(thirtysixhoursago)\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Filter by tag\n",
    "\n",
    "Extract the temperature data from the cooking base area (sensor S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S1|        290|      30|2021-02-26 15:33:26|\n",
      "|    S1|        305|      38|2021-02-26 15:33:36|\n",
      "|    S1|        280|      45|2021-02-26 15:33:45|\n",
      "|    S1|        280|      22|2021-02-26 15:33:54|\n",
      "|    S1|        285|      32|2021-02-26 15:34:03|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE sensor = 'S1'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4 - Filter By Value \n",
    "\n",
    "Extract the measurements from the cooking base area (sensor S1) with a temperature under 300°  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|                 ts|\n",
      "+------+-----------+--------+-------------------+\n",
      "|    S1|        290|      30|2021-02-26 15:33:26|\n",
      "|    S1|        280|      45|2021-02-26 15:33:45|\n",
      "|    S1|        280|      22|2021-02-26 15:33:54|\n",
      "|    S1|        285|      32|2021-02-26 15:34:03|\n",
      "|    S1|        290|      30|2021-02-27 09:20:27|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM temperature_humiditySensorEvent WHERE sensor = 'S1' AND temperature < 300 \").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 - Grouping + Aggregator (mean)\n",
    "\n",
    "#### Extract the average temperature and the average humidity along the different stages of the linear pizza oven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Watermarks are necessary while quering the data, in order to understand how much the data can arrive late \n",
    "All_time_averages_query = (decoded_streaming_temperature_humidity_df\n",
    "                         .withWatermark(\"ts\", \"1 minutes\")\n",
    "                         .groupBy(col(\"sensor\"))\n",
    "                         .avg(\"humidity\", \"temperature\")\n",
    "                     .writeStream\n",
    "                     .outputMode(\"complete\")\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"group_query3\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----------------+\n",
      "|sensor|avg(humidity)|avg(temperature)|\n",
      "+------+-------------+----------------+\n",
      "|    S2|         61.9|           106.5|\n",
      "|    S1|         33.4|           288.0|\n",
      "+------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The execution of this query could require some time: if dataframe seems empty, just try to re-run the cell after a while\n",
    "spark.sql(\"SELECT * FROM group_query3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_time_averages_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the last humidity and temperature measurements from the cooking base area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+--------------------+\n",
      "|sensor|temperature|humidity|                  ts|\n",
      "+------+-----------+--------+--------------------+\n",
      "|    S1|        285|      32|+105664-03-19 10:...|\n",
      "+------+-----------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AND is necessary because there can be a record with that ts also from the s2 sensor\n",
    "spark.sql(\"\"\"SELECT * FROM temperature_humiditySensorEvent WHERE ts = (SELECT MAX(ts) FROM temperature_humiditySensorEvent \n",
    "            WHERE sensor = 'S1') AND sensor = 'S1'\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 - Aggregate Window\n",
    "\n",
    "#### Extract the moving average temperature observed in the cooking base area over a window of 2 minutes (DEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: this corresponds to a logical tumbling window\n",
    "LTW_temperature_query = (decoded_streaming_temperature_humidity_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"2 minutes\"),\"sensor\")\n",
    "                         .avg(\"temperature\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+----------------+\n",
      "|window                                    |sensor|avg(temperature)|\n",
      "+------------------------------------------+------+----------------+\n",
      "|[2020-07-21 12:00:00, 2020-07-21 12:02:00]|S1    |297.5           |\n",
      "|[2020-07-21 12:02:00, 2020-07-21 12:04:00]|S1    |280.0           |\n",
      "|[2020-07-21 12:04:00, 2020-07-21 12:06:00]|S1    |285.0           |\n",
      "|[2021-02-26 15:32:00, 2021-02-26 15:34:00]|S1    |288.75          |\n",
      "|[2021-02-26 15:34:00, 2021-02-26 15:36:00]|S1    |285.0           |\n",
      "+------------------------------------------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results WHERE sensor = 'S1' ORDER BY window ASC\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the moving average temperature observed by S2 over a window of 3 minutes (hands-on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query2 = (decoded_streaming_temperature_humidity_df\n",
    "                         .withWatermark(\"TS\", \"1 minutes\")\n",
    "                         .groupBy(window(\"TS\", \"3 minutes\"),\"sensor\")\n",
    "                         .avg(\"temperature\")\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results2\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+-----------------+\n",
      "|window                                    |sensor|avg(temperature) |\n",
      "+------------------------------------------+------+-----------------+\n",
      "|[2020-07-21 12:00:00, 2020-07-21 12:03:00]|S2    |112.5            |\n",
      "|[2020-07-21 12:03:00, 2020-07-21 12:06:00]|S2    |97.5             |\n",
      "|[2021-02-26 15:33:00, 2021-02-26 15:36:00]|S2    |106.5            |\n",
      "|[2021-02-27 09:18:00, 2021-02-27 09:21:00]|S2    |110.0            |\n",
      "|[2021-02-27 09:21:00, 2021-02-27 09:24:00]|S2    |98.33333333333333|\n",
      "+------------------------------------------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results2 WHERE sensor = 'S2' ORDER BY window ASC\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTW_temperature_query2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 - Map and custom function\n",
    "\n",
    "#### Correct the temperature observations of the cooking base area by by subtracting a delta of 5°C to each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to keep all the records, also the one from the other sensor, a solution could be:\n",
    "\n",
    "new_column = when(\n",
    "        (col(\"sensor\") == \"S1\"), col(\"temperature\") - 5\n",
    "    ).otherwise(col(\"temperature\"))\n",
    "\n",
    "map_temperature_query = (decoded_streaming_temperature_humidity_df\n",
    "                         .withColumn(\"temperature\", new_column)\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|ts                 |\n",
      "+------+-----------+--------+-------------------+\n",
      "|S1    |285        |30      |2021-02-26 15:33:26|\n",
      "|S2    |105        |55      |2021-02-26 15:33:30|\n",
      "|S2    |110        |60      |2021-02-26 15:33:33|\n",
      "|S1    |300        |38      |2021-02-26 15:33:36|\n",
      "|S2    |120        |65      |2021-02-26 15:33:39|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively, if you'd like to keep only the values from sensor S1 a solution could be:\n",
    "def sub5(x):\n",
    "    x = x-5\n",
    "    return x\n",
    "\n",
    "df = decoded_streaming_temperature_humidity_df.select(\"*\").where(\"sensor = 'S1'\")\n",
    "fun = udf(sub5)\n",
    "\n",
    "map_temperature_query = (df\n",
    "                         .withColumn(\"temperature\", fun(df[\"temperature\"]))\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+\n",
      "|sensor|temperature|humidity|ts                 |\n",
      "+------+-----------+--------+-------------------+\n",
      "|S1    |285        |30      |2021-02-26 15:33:26|\n",
      "|S1    |300        |38      |2021-02-26 15:33:36|\n",
      "|S1    |275        |45      |2021-02-26 15:33:45|\n",
      "|S1    |275        |22      |2021-02-26 15:33:54|\n",
      "|S1    |280        |32      |2021-02-26 15:34:03|\n",
      "+------+-----------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results\").show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_temperature_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 - Stream-to-Stream Join\n",
    "\n",
    "#### Extract the difference between the temperature of the base cooking area and the mozzarella melting area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join assuming synchronous time-series\n",
    "\n",
    "Apply watermarks on event-time columns and other filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_events = (decoded_streaming_temperature_humidity_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"sensor\") == \"S1\")\n",
    "               )\n",
    "\n",
    "only_S2_events = (decoded_streaming_temperature_humidity_df\n",
    "                .withWatermark(\"ts\", \"1 minute\")\n",
    "                .filter(col(\"sensor\") == \"S2\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with event-time constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = (only_S1_events.join(\n",
    "  only_S2_events,\n",
    "    (only_S1_events.ts == only_S2_events.ts)) \n",
    "           .select(only_S1_events.temperature,\n",
    "                   only_S2_events.temperature,\n",
    "                   only_S1_events.humidity,\n",
    "                   only_S2_events.humidity,\n",
    "                   only_S1_events.ts\n",
    "                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query = (join_df\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------+--------+---+\n",
      "|temperature|temperature|humidity|humidity|ts |\n",
      "+-----------+-----------+--------+--------+---+\n",
      "+-----------+-----------+--------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM results ORDER BY ts DESC\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** If we simply try to join on the ts the df will always be empty, since the records are not sincronized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_to_s_join_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join assuming a fixed delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S1\")\n",
    "                .select(col(\"ts\").alias(\"S1_ts\"), \n",
    "                        col(\"temperature\").alias(\"S1_temperature\"), col(\"humidity\").alias(\"S1_humidity\"))\n",
    "                .withWatermark(\"S1_ts\", \"2 hours\")\n",
    "               )\n",
    "\n",
    "only_S2_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S2\")\n",
    "                .select(col(\"ts\").alias(\"S2_ts\"), \n",
    "                        col(\"temperature\").alias(\"S2_temperature\"), col(\"humidity\").alias(\"S2_humidity\"))\n",
    "                .withWatermark(\"S2_ts\", \"2 hours\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_query = (only_S1_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results1\")\n",
    "                     .start())\n",
    "\n",
    "only_S2_query = (only_S2_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results2\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------+-------------------+--------------+-----------+\n",
      "|              S1_ts|S1_temperature|S1_humidity|              S2_ts|S2_temperature|S2_humidity|\n",
      "+-------------------+--------------+-----------+-------------------+--------------+-----------+\n",
      "|2021-02-26 15:33:36|           305|         38|2021-02-26 15:33:33|           110|         60|\n",
      "|2021-02-26 15:33:45|           280|         45|2021-02-26 15:33:42|           115|         60|\n",
      "|2021-02-26 15:33:54|           280|         22|2021-02-26 15:33:51|           115|         72|\n",
      "|2021-02-26 15:34:03|           285|         32|2021-02-26 15:34:00|            90|         60|\n",
      "|2021-02-27 09:20:37|           305|         38|2021-02-27 09:20:34|           110|         60|\n",
      "|2021-02-27 09:20:46|           280|         45|2021-02-27 09:20:43|           115|         60|\n",
      "|2021-02-27 09:20:55|           280|         22|2021-02-27 09:20:52|           115|         72|\n",
      "|2021-02-27 09:21:04|           285|         32|2021-02-27 09:21:01|            90|         60|\n",
      "|2021-03-01 11:36:52|           305|         38|2021-03-01 11:36:49|           110|         60|\n",
      "|2021-03-01 11:37:01|           280|         45|2021-03-01 11:36:58|           115|         60|\n",
      "|2021-03-01 11:37:10|           280|         22|2021-03-01 11:37:07|           115|         72|\n",
      "|2021-03-01 11:37:19|           285|         32|2021-03-01 11:37:16|            90|         60|\n",
      "|2021-03-01 20:05:36|           305|         38|2021-03-01 20:05:33|           110|         60|\n",
      "|2021-03-01 20:05:45|           280|         45|2021-03-01 20:05:42|           115|         60|\n",
      "|2021-03-01 20:05:54|           280|         22|2021-03-01 20:05:51|           115|         72|\n",
      "|2021-03-01 20:06:03|           285|         32|2021-03-01 20:06:00|            90|         60|\n",
      "+-------------------+--------------+-----------+-------------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join \n",
    "df = spark.sql(\"SELECT * FROM results1 join results2 ON S1_ts <= (S2_ts + INTERVAL 4 seconds) AND S1_ts >= S2_ts\")\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-----------+-------------------+--------------+-----------+----------+\n",
      "|              S1_ts|S1_temperature|S1_humidity|              S2_ts|S2_temperature|S2_humidity|difference|\n",
      "+-------------------+--------------+-----------+-------------------+--------------+-----------+----------+\n",
      "|2021-02-26 15:33:36|           305|         38|2021-02-26 15:33:33|           110|         60|       195|\n",
      "|2021-02-26 15:33:45|           280|         45|2021-02-26 15:33:42|           115|         60|       165|\n",
      "|2021-02-26 15:33:54|           280|         22|2021-02-26 15:33:51|           115|         72|       165|\n",
      "|2021-02-26 15:34:03|           285|         32|2021-02-26 15:34:00|            90|         60|       195|\n",
      "|2021-02-27 09:20:37|           305|         38|2021-02-27 09:20:34|           110|         60|       195|\n",
      "|2021-02-27 09:20:46|           280|         45|2021-02-27 09:20:43|           115|         60|       165|\n",
      "|2021-02-27 09:20:55|           280|         22|2021-02-27 09:20:52|           115|         72|       165|\n",
      "|2021-02-27 09:21:04|           285|         32|2021-02-27 09:21:01|            90|         60|       195|\n",
      "|2021-03-01 11:36:52|           305|         38|2021-03-01 11:36:49|           110|         60|       195|\n",
      "|2021-03-01 11:37:01|           280|         45|2021-03-01 11:36:58|           115|         60|       165|\n",
      "|2021-03-01 11:37:10|           280|         22|2021-03-01 11:37:07|           115|         72|       165|\n",
      "|2021-03-01 11:37:19|           285|         32|2021-03-01 11:37:16|            90|         60|       195|\n",
      "|2021-03-01 20:05:36|           305|         38|2021-03-01 20:05:33|           110|         60|       195|\n",
      "|2021-03-01 20:05:45|           280|         45|2021-03-01 20:05:42|           115|         60|       165|\n",
      "|2021-03-01 20:05:54|           280|         22|2021-03-01 20:05:51|           115|         72|       165|\n",
      "|2021-03-01 20:06:03|           285|         32|2021-03-01 20:06:00|            90|         60|       195|\n",
      "+-------------------+--------------+-----------+-------------------+--------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def diff(x, y):\n",
    "    return x - y\n",
    "\n",
    "fun = udf(diff)\n",
    "\n",
    "#Calculating difference\n",
    "df.withColumn(\"difference\", fun(df[\"S1_temperature\"], df[\"S2_temperature\"])).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S2_query.stop()\n",
    "only_S1_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join exploiting time-windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_wind_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S1\")\n",
    "                .select(col(\"ts\").alias(\"S1_ts\"), \n",
    "                        col(\"temperature\").alias(\"S1_temperature\"), col(\"humidity\").alias(\"S1_humidity\"))\n",
    "                .withWatermark(\"S1_ts\", \"2 hours\")\n",
    "                       .groupBy(window(\"S1_ts\", \"10 seconds\"))\n",
    "                       .avg(\"S1_humidity\")\n",
    "               )\n",
    "\n",
    "only_S2_wind_events = (decoded_streaming_temperature_humidity_df\n",
    "                .filter(col(\"sensor\") == \"S2\")\n",
    "                .select(col(\"ts\").alias(\"S2_ts\"), \n",
    "                        col(\"temperature\").alias(\"S2_temperature\"), col(\"humidity\").alias(\"S2_humidity\"))\n",
    "                .withWatermark(\"S2_ts\", \"2 hours\")\n",
    "                       .groupBy(window(\"S2_ts\", \"10 seconds\"))\n",
    "                       .avg(\"S2_humidity\")\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_wind_query = (only_S1_wind_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results1\")\n",
    "                     .start())\n",
    "\n",
    "only_S2_wind_query = (only_S2_wind_events\n",
    "                     .writeStream\n",
    "                     .format(\"memory\")\n",
    "                     .queryName(\"results2\")\n",
    "                     .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+------------------+\n",
      "|              window|avg(S1_humidity)|              window|  avg(S2_humidity)|\n",
      "+--------------------+----------------+--------------------+------------------+\n",
      "|[2021-02-27 09:21...|            32.0|[2021-02-27 09:21...|              57.5|\n",
      "|[2021-02-26 15:34...|            32.0|[2021-02-26 15:34...|58.333333333333336|\n",
      "|[2021-03-01 20:05...|            22.0|[2021-03-01 20:05...|              68.5|\n",
      "|[2021-02-26 15:33...|            38.0|[2021-02-26 15:33...|              60.0|\n",
      "|[2021-02-27 09:20...|            38.0|[2021-02-27 09:20...|              57.5|\n",
      "|[2021-03-01 20:06...|            32.0|[2021-03-01 20:06...|58.333333333333336|\n",
      "|[2021-03-01 11:36...|            30.0|[2021-03-01 11:36...|              57.5|\n",
      "|[2021-02-26 15:33...|            22.0|[2021-02-26 15:33...|              68.5|\n",
      "|[2021-03-01 20:05...|            45.0|[2021-03-01 20:05...|              63.5|\n",
      "|[2021-02-27 09:20...|            22.0|[2021-02-27 09:20...|              68.5|\n",
      "|[2021-03-01 11:36...|            38.0|[2021-03-01 11:36...|              62.5|\n",
      "|[2021-03-01 20:05...|            38.0|[2021-03-01 20:05...|              60.0|\n",
      "|[2021-02-26 15:33...|            45.0|[2021-02-26 15:33...|              63.5|\n",
      "|[2021-02-27 09:20...|            45.0|[2021-02-27 09:20...|              64.0|\n",
      "|[2021-03-01 11:37...|            45.0|[2021-03-01 11:37...|              69.5|\n",
      "|[2021-03-01 11:37...|            27.0|[2021-03-01 11:37...|              62.5|\n",
      "+--------------------+----------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join \n",
    "df = spark.sql(\"SELECT * FROM results1 join results2 ON results1.window = results2.window\")\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+------------------+------------------+\n",
      "|              window|avg(S1_humidity)|              window|  avg(S2_humidity)|        difference|\n",
      "+--------------------+----------------+--------------------+------------------+------------------+\n",
      "|[2021-02-27 09:21...|            32.0|[2021-02-27 09:21...|              57.5|              25.5|\n",
      "|[2021-02-26 15:34...|            32.0|[2021-02-26 15:34...|58.333333333333336|26.333333333333336|\n",
      "|[2021-03-01 20:05...|            22.0|[2021-03-01 20:05...|              68.5|              46.5|\n",
      "|[2021-02-26 15:33...|            38.0|[2021-02-26 15:33...|              60.0|              22.0|\n",
      "|[2021-02-27 09:20...|            38.0|[2021-02-27 09:20...|              57.5|              19.5|\n",
      "|[2021-03-01 20:06...|            32.0|[2021-03-01 20:06...|58.333333333333336|26.333333333333336|\n",
      "|[2021-03-01 11:36...|            30.0|[2021-03-01 11:36...|              57.5|              27.5|\n",
      "|[2021-02-26 15:33...|            22.0|[2021-02-26 15:33...|              68.5|              46.5|\n",
      "|[2021-03-01 20:05...|            45.0|[2021-03-01 20:05...|              63.5|              18.5|\n",
      "|[2021-02-27 09:20...|            22.0|[2021-02-27 09:20...|              68.5|              46.5|\n",
      "|[2021-03-01 11:36...|            38.0|[2021-03-01 11:36...|              62.5|              24.5|\n",
      "|[2021-03-01 20:05...|            38.0|[2021-03-01 20:05...|              60.0|              22.0|\n",
      "|[2021-02-26 15:33...|            45.0|[2021-02-26 15:33...|              63.5|              18.5|\n",
      "|[2021-02-27 09:20...|            45.0|[2021-02-27 09:20...|              64.0|              19.0|\n",
      "|[2021-03-01 11:37...|            45.0|[2021-03-01 11:37...|              69.5|              24.5|\n",
      "|[2021-03-01 11:37...|            27.0|[2021-03-01 11:37...|              62.5|              35.5|\n",
      "+--------------------+----------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculating difference\n",
    "df = df.withColumn(\"difference\", fun(df[\"avg(S2_humidity)\"], df[\"avg(S1_humidity)\"]))\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the difference between the humidity levels of the base cooking area and the mozzarella melting area. Find if the differences are between 20 and 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+------------------+------------------+\n",
      "|              window|avg(S1_humidity)|              window|  avg(S2_humidity)|        difference|\n",
      "+--------------------+----------------+--------------------+------------------+------------------+\n",
      "|[2021-02-27 09:21...|            32.0|[2021-02-27 09:21...|              57.5|              25.5|\n",
      "|[2021-02-26 15:34...|            32.0|[2021-02-26 15:34...|58.333333333333336|26.333333333333336|\n",
      "|[2021-02-26 15:33...|            38.0|[2021-02-26 15:33...|              60.0|              22.0|\n",
      "|[2021-03-01 20:06...|            32.0|[2021-03-01 20:06...|58.333333333333336|26.333333333333336|\n",
      "|[2021-03-01 11:36...|            30.0|[2021-03-01 11:36...|              57.5|              27.5|\n",
      "+--------------------+----------------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df[\"difference\"] > 20).filter(df[\"difference\"] < 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_S1_wind_query.stop()\n",
    "only_S2_wind_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 - static-streaming join df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the following data are store in a DB\n",
    "\n",
    "```\n",
    "CREATE DATABASE pizza-erp;\n",
    "\n",
    "CREATE TABLE public.oven\n",
    "(\n",
    "    pid bigint NOT NULL,\n",
    "    kind character varying COLLATE pg_catalog.\"default\" NOT NULL,\n",
    "    enteringtime bigint NOT NULL,\n",
    "    exitingtime bigint,\n",
    "    sensor character varying COLLATE pg_catalog.\"default\" NOT NULL,\n",
    "    CONSTRAINT hoven_pkey PRIMARY KEY (pid,enteringtime,sensor)\n",
    ");\n",
    "\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(2,'napoli',1602504000000000000,1602504150000000000,'S1');\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(1,'margherita',1602504010000000000,1602504080000000000,'S2');\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(3,'pepperoni',1602504170000000000,1602504250000000000,'S1');\n",
    "INSERT INTO oven (pid,kind,enteringtime,exitingtime,sensor) VALUES(2,'napoli',1602504130000000000,1602504284000000000,'S2');\n",
    "```\n",
    "\n",
    "enrich the time-serires with the data in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "#create the static-df\n",
    "pizza_df = sc.parallelize([\n",
    "    [2,'napoli', 1595332780,1595332820,'S1'],\n",
    "    [1,'margherita',1595332795,1595332835,'S2'],\n",
    "    [3,'pepperoni',1595332840,1595332880,'S1'],\n",
    "    [2,'napoli',1595332825,1595332865,'S2']]\n",
    ").toDF([\"pid\",\"kind\",\"enteringtime\",\"exitingtime\",\"sensor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-------------------+------+\n",
      "|pid|      kind|       enteringtime|        exitingtime|sensor|\n",
      "+---+----------+-------------------+-------------------+------+\n",
      "|  2|    napoli|2020-07-21 11:59:40|2020-07-21 12:00:20|    S1|\n",
      "|  1|margherita|2020-07-21 11:59:55|2020-07-21 12:00:35|    S2|\n",
      "|  3| pepperoni|2020-07-21 12:00:40|2020-07-21 12:01:20|    S1|\n",
      "|  2|    napoli|2020-07-21 12:00:25|2020-07-21 12:01:05|    S2|\n",
      "+---+----------+-------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cast time from unix to ts format\n",
    "\n",
    "pizza_df = pizza_df.withColumn(\"enteringtime\", to_timestamp(pizza_df[\"enteringtime\"]))\n",
    "pizza_df = pizza_df.withColumn(\"exitingtime\", to_timestamp(pizza_df[\"exitingtime\"]))\n",
    "pizza_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = decoded_streaming_temperature_humidity_df.join(pizza_df, (pizza_df.sensor == decoded_streaming_temperature_humidity_df.sensor) & \n",
    "                                                         (pizza_df.enteringtime <= decoded_streaming_temperature_humidity_df.ts) & \n",
    "                                                         (pizza_df.exitingtime >= decoded_streaming_temperature_humidity_df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_query = (join_df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"join_Event\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+-------------------+---+----------+-------------------+-------------------+------+\n",
      "|sensor|temperature|humidity|                 ts|pid|      kind|       enteringtime|        exitingtime|sensor|\n",
      "+------+-----------+--------+-------------------+---+----------+-------------------+-------------------+------+\n",
      "|    S2|        105|      55|2020-07-21 12:00:15|  1|margherita|2020-07-21 11:59:55|2020-07-21 12:00:35|    S2|\n",
      "|    S2|        110|      60|2020-07-21 12:00:45|  2|    napoli|2020-07-21 12:00:25|2020-07-21 12:01:05|    S2|\n",
      "|    S2|        105|      55|2020-07-21 12:00:15|  1|margherita|2020-07-21 11:59:55|2020-07-21 12:00:35|    S2|\n",
      "|    S2|        110|      60|2020-07-21 12:00:45|  2|    napoli|2020-07-21 12:00:25|2020-07-21 12:01:05|    S2|\n",
      "|    S1|        290|      30|2020-07-21 12:00:00|  2|    napoli|2020-07-21 11:59:40|2020-07-21 12:00:20|    S1|\n",
      "|    S1|        305|      38|2020-07-21 12:01:00|  3| pepperoni|2020-07-21 12:00:40|2020-07-21 12:01:20|    S1|\n",
      "|    S1|        290|      30|2020-07-21 12:00:00|  2|    napoli|2020-07-21 11:59:40|2020-07-21 12:00:20|    S1|\n",
      "|    S1|        305|      38|2020-07-21 12:01:00|  3| pepperoni|2020-07-21 12:00:40|2020-07-21 12:01:20|    S1|\n",
      "+------+-----------+--------+-------------------+---+----------+-------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM join_Event\")\n",
    "df.show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_humidity_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
